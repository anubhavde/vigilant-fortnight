{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True  # fire on all cylinders\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All patch triggers in the dataset are rectangular, so we will regress to the top-left and bottom-right corners instead of predicting the segmentation mask directly. Directly prediction the segmentation mask is another valid option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDatasetTriggerSynthesis(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_folder):\n",
    "        super().__init__()\n",
    "        model_paths = [os.path.join(model_folder, x) for x in sorted(os.listdir(os.path.join(model_folder)))]\n",
    "        coords = []\n",
    "        masks = []\n",
    "        data_sources = []\n",
    "        for p in model_paths:\n",
    "            with open(os.path.join(p, 'info.json'), 'r') as f:\n",
    "                info = json.load(f)\n",
    "                data_sources.append(info['dataset'])\n",
    "            attack_specification = torch.load(os.path.join(p, 'attack_specification.pt'))\n",
    "            trigger = attack_specification['trigger']\n",
    "            masks.append(trigger['mask'])\n",
    "            ul = trigger['top_left']\n",
    "            br = trigger['bottom_right']\n",
    "            coords.append(np.stack([ul, br]))\n",
    "            \n",
    "        self.model_paths = model_paths\n",
    "        self.coords = coords\n",
    "        self.masks = masks\n",
    "        self.data_sources = data_sources\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.model_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.load(os.path.join(self.model_paths[index], 'model.pt')), \\\n",
    "               self.coords[index], self.masks[index], self.data_sources[index]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    return [x[0] for x in batch], [x[1] for x in batch], [x[2] for x in batch], [x[3] for x in batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Spliting off a validation set from the train set for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../tdc_datasets'\n",
    "task = 'trigger_synthesis'\n",
    "dataset = NetworkDatasetTriggerSynthesis(os.path.join(dataset_path, task, 'train'))\n",
    "\n",
    "split = int(len(dataset) * 0.8)\n",
    "rnd_idx = np.random.permutation(len(dataset))\n",
    "train_dataset = torch.utils.data.Subset(dataset, rnd_idx[:split])\n",
    "val_dataset = torch.utils.data.Subset(dataset, rnd_idx[split:])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True,\n",
    "                                           num_workers=0, pin_memory=False, collate_fn=custom_collate)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1,\n",
    "                                           num_workers=0, pin_memory=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the MNTD network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = ['CIFAR-10', 'CIFAR-100', 'GTSRB', 'MNIST']\n",
    "data_source_to_channel = {k: 1 if k == 'MNIST' else 3 for k in data_sources}\n",
    "data_source_to_resolution = {k: 28 if k == 'MNIST' else 32 for k in data_sources}\n",
    "data_source_to_num_classes = {'CIFAR-10': 10, 'CIFAR-100': 100, 'GTSRB': 43, 'MNIST': 10}\n",
    "\n",
    "class MetaNetwork(nn.Module):\n",
    "    def __init__(self, num_queries, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.queries = nn.ParameterDict(\n",
    "            {k: nn.Parameter(torch.rand(num_queries,\n",
    "                                        data_source_to_channel[k],\n",
    "                                        data_source_to_resolution[k],\n",
    "                                        data_source_to_resolution[k])) for k in data_sources}\n",
    "        )\n",
    "        self.affines = nn.ModuleDict(\n",
    "            {k: nn.Linear(data_source_to_num_classes[k]*num_queries, 32) for k in data_sources}\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(32)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.final_output = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, net, data_source):\n",
    "        \"\"\"\n",
    "        :param net: an input network of one of the model_types specified at init\n",
    "        :param data_source: the name of the data source\n",
    "        :returns: a score for whether the network is a Trojan or not\n",
    "        \"\"\"\n",
    "        query = self.queries[data_source]\n",
    "        out = net(query)\n",
    "        out = self.affines[data_source](out.view(1, -1))\n",
    "        out = self.norm(out)\n",
    "        out = self.relu(out)\n",
    "        return self.final_output(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91fee76816b4c2da896f5c1bcf783d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653f345253994e9d96721bb5d168e508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c27ab39988e4de782652f8eeb406fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9fac91753849c9b262fc75258da1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0570bc6376964cde8c08cb114263ec1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e2b96ea1f94dafa14a901f60f2c3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110a10da15c9425f944fff52078f35ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa8b60bb6aa4870bf244bcb5038fc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e36aff78f5046bbaa92bbc0ac30f2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e64fbc8ed21434d96b3399e0fb36393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_network = MetaNetwork(10, num_classes=4).cuda().train()\n",
    "\n",
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "weight_decay = 0.\n",
    "optimizer = torch.optim.Adam(meta_network.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs * len(train_dataset))\n",
    "\n",
    "loss_ema = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    pbar = tqdm(train_loader)\n",
    "    pbar.set_description(f\"Epoch {epoch + 1}\")\n",
    "    for i, (net, coords, mask, data_source) in enumerate(pbar):\n",
    "        net = net[0]\n",
    "        coords = torch.FloatTensor(coords[0]).view(-1).cuda()\n",
    "        data_source = data_source[0]\n",
    "        net.cuda().eval()\n",
    "        \n",
    "        out = meta_network(net, data_source)\n",
    "        \n",
    "        loss = (out - coords.cuda()).pow(2).sum().pow(0.5)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(inputs=list(meta_network.parameters()))\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        for k in meta_network.queries.keys():\n",
    "            meta_network.queries[k].data = meta_network.queries[k].data.clamp(0, 1)\n",
    "        loss_ema = loss.item() if loss_ema == np.inf else 0.95 * loss_ema + 0.05 * loss.item()\n",
    "\n",
    "        pbar.set_postfix(loss=loss_ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaNetwork(\n",
       "  (queries): ParameterDict(\n",
       "      (CIFAR-10): Parameter containing: [torch.cuda.FloatTensor of size 10x3x32x32 (GPU 0)]\n",
       "      (CIFAR-100): Parameter containing: [torch.cuda.FloatTensor of size 10x3x32x32 (GPU 0)]\n",
       "      (GTSRB): Parameter containing: [torch.cuda.FloatTensor of size 10x3x32x32 (GPU 0)]\n",
       "      (MNIST): Parameter containing: [torch.cuda.FloatTensor of size 10x1x28x28 (GPU 0)]\n",
       "  )\n",
       "  (affines): ModuleDict(\n",
       "    (CIFAR-10): Linear(in_features=100, out_features=32, bias=True)\n",
       "    (CIFAR-100): Linear(in_features=1000, out_features=32, bias=True)\n",
       "    (GTSRB): Linear(in_features=430, out_features=32, bias=True)\n",
       "    (MNIST): Linear(in_features=100, out_features=32, bias=True)\n",
       "  )\n",
       "  (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (final_output): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_rectangular_mask\n",
    "\n",
    "data_source_to_resolution = {k: 28 if k == 'MNIST' else 32 for k in data_sources}\n",
    "def is_valid_rectangle(top_left, bottom_right):\n",
    "    \"\"\"\n",
    "    returns True if the top_left and bottom_right coords define a rectangle\n",
    "    \"\"\"\n",
    "    if (top_left[0] < bottom_right[0]) and (top_left[1] < bottom_right[1]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def evaluate(meta_network, loader):\n",
    "    loss_list = []\n",
    "    \n",
    "    for i, (net, coords, mask, data_source) in enumerate(tqdm(loader)):\n",
    "        net[0].cuda().eval()\n",
    "        coords = torch.FloatTensor(coords[0]).view(-1).cuda()\n",
    "        with torch.no_grad():\n",
    "            out = meta_network(net[0], data_source[0])\n",
    "        loss = (out - coords).pow(2).sum().pow(0.5)\n",
    "        loss_list.append(loss.item())\n",
    "    \n",
    "    return np.mean(loss_list)\n",
    "\n",
    "def evaluate_iou(meta_network, loader):\n",
    "    iou_list = []\n",
    "    \n",
    "    for i, (net, coords, mask, data_source) in enumerate(tqdm(loader)):\n",
    "        net[0].cuda().eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = meta_network(net[0], data_source[0])\n",
    "            pred_top_left = np.rint(out[0, :2].cpu().numpy()).astype(np.int32)  # rounding to integer array\n",
    "            pred_bottom_right = np.rint(out[0, 2:].cpu().numpy()).astype(np.int32)  # rounding to integer array\n",
    "        \n",
    "        # convert predicted coordinates to mask\n",
    "        side_len = data_source_to_resolution[data_source[0]]\n",
    "        if is_valid_rectangle(pred_top_left, pred_bottom_right):\n",
    "            pred_mask = create_rectangular_mask(side_len, pred_top_left, pred_bottom_right)\n",
    "        else:\n",
    "            # default to all-ones mask if the prediction is not a valid rectangle\n",
    "            pred_mask = create_rectangular_mask(side_len, [0,0], [side_len, side_len])\n",
    "            print('hiya!')\n",
    "        \n",
    "        # compute IoU with true mask\n",
    "        mask = mask[0].numpy().astype(np.int32)\n",
    "        union = np.logical_or(pred_mask, mask).sum()\n",
    "        intersection = np.logical_and(pred_mask, mask).sum()\n",
    "        if union == 0:\n",
    "            iou = 0\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "            \n",
    "        iou_list.append(iou)\n",
    "    \n",
    "    return np.mean(iou_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591f19bc46594254960b118efaf22f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f2acb394694cc082b6057ec7dd1ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.104, Train IOU: 0.378\n"
     ]
    }
   ],
   "source": [
    "loss, iou = evaluate(meta_network, train_loader), evaluate_iou(meta_network, train_loader)\n",
    "print(f'Train Loss: {loss:.3f}, Train IOU: {iou:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50257aab968942d3a1cfb352a5420145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea81cbc04fd5450394b6cf6de8d0b712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 14.422, Val IOU: 0.044\n"
     ]
    }
   ],
   "source": [
    "loss, iou = evaluate(meta_network, val_loader), evaluate_iou(meta_network, val_loader)\n",
    "print(f'Val Loss: {loss:.3f}, Val IOU: {iou:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDatasetTriggerSynthesisTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_folder):\n",
    "        super().__init__()\n",
    "        model_paths = [os.path.join(model_folder, x) for x in sorted(os.listdir(os.path.join(model_folder)))]\n",
    "        data_sources = []\n",
    "        for p in model_paths:\n",
    "            with open(os.path.join(p, 'info.json'), 'r') as f:\n",
    "                info = json.load(f)\n",
    "                data_sources.append(info['dataset'])\n",
    "            \n",
    "        self.model_paths = model_paths\n",
    "        self.data_sources = data_sources\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.model_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.load(os.path.join(self.model_paths[index], 'model.pt')), self.data_sources[index]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    return [x[0] for x in batch], [x[1] for x in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../tdc_datasets'\n",
    "task = 'trigger_synthesis'\n",
    "\n",
    "test_dataset = NetworkDatasetTriggerSynthesisTest(os.path.join(dataset_path, task, 'val'))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
    "                                          num_workers=0, pin_memory=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(meta_network, loader):\n",
    "    masks = []\n",
    "    \n",
    "    for i, (net, data_source) in enumerate(tqdm(loader)):\n",
    "        net[0].cuda().eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = meta_network(net[0], data_source[0])\n",
    "            pred_top_left = np.rint(out[0, :2].cpu().numpy()).astype(np.int32)  # rounding to integer array\n",
    "            pred_bottom_right = np.rint(out[0, 2:].cpu().numpy()).astype(np.int32)  # rounding to integer array\n",
    "        \n",
    "        if is_valid_rectangle(pred_top_left, pred_bottom_right):\n",
    "            side_len = data_source_to_resolution[data_source[0]]\n",
    "            pred_mask = create_rectangular_mask(side_len, pred_top_left, pred_bottom_right)\n",
    "            masks.append(pred_mask.numpy().astype(bool))\n",
    "        else:\n",
    "            # as a heuristic, we output an all-ones mask if the predicted corners do not form a valid rectangle\n",
    "            pred_mask = create_rectangular_mask(side_len, [0,0], [side_len,side_len])\n",
    "            masks.append(pred_mask.numpy().astype(bool))\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c12a2670da4411ab83b875a12cbf617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "masks = predict(meta_network, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: predictions.pkl (deflated 99%)\r\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('mntd_submission'):\n",
    "    os.makedirs('mntd_submission')\n",
    "\n",
    "with open(os.path.join('mntd_submission', 'predictions.pkl'), 'wb') as f:\n",
    "    pickle.dump(masks, f)\n",
    "\n",
    "!cd mntd_submission && zip ../mntd_submission.zip ./* && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_submission.ipynb  mntd_submission  mntd_submission.zip\tREADME.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "cfe34a9389bfb9158f4a57d38254999ecb4846a6b929cd8c17eb23c1b8c530ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
